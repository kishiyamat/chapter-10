---
title: "Generalized Linear Model 1"
author: "Takeshi Kishiyama"
date: '`r format(Sys.time(), "%Y/%m/%d %H:%M")`'
output: ioslides_presentation
# output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    message = FALSE,
    cache = FALSE,
    warning = FALSE,
    width=6,
    fig.align='center',
    fig.pos='H',
    dev = c("png", "svg")
)
```

# はじめに

## GLM

一般化線形モデル、GLM(Generalized Linear Model)とは？ <br>
(教科書のp.141)

* 説明変数 $(x_{i1}, x_{i2}, ..., x_{ik})$ と
  切片 $\beta_0$ 、傾き ($\beta_1, \beta_2, ... \beta_k$) で
  応答変数 $y_i$ をモデルする。
* $f(y_i) = z_i =\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... \beta_kx_{ik} + \epsilon$
* 左辺はリンク関数 $f(y_i)$ で 右辺の $z_i$ を線形予測子と呼ぶ。
* なお、$f(y)$ の $y$ は応答変数 $y$ が従う確率分布のパラメターとする。

なるほど（わからん）

## GLM

とりあえず、わからないものたちを列挙

* $f(y_i) = z_i =\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... \beta_kx_{ik} + \epsilon$
    * $y_i$ とか $x_i$ とかわからん。
    * 切片 $\beta_0$ 、傾き ($\beta_1, \beta_2, ... \beta_k$) ってなに？
    * モデルするってどういう意味？
* 左辺はリンク関数 $f(y_i)$ で 右辺の $z_i$ を線形予測子と呼ぶ。
    * 関数わからん。読み終わった後に「で、リンク関数って何？」ってなる。
* なお、$f(y)$ の $y$ は応答変数 $y$ が従う確率分布のパラメターとする。
    * で結局$y$は何なの？

## GLM

難しい話を順序だてて整理していく<br>
$f(y_i) = z_i =\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... \beta_kx_{ik} + \epsilon$

* 式全体(関数、$y_i$ や $x_i$ i.e. ベクトル)、推定
    * $\to$ R入門の章で片付けます。
* 説明変数、応答変数
    * $\to$ 8,9章で片付けます。
* 線形予測子、リンク関数
    * 10章で片付けます。

質問は[こちら](https://forms.gle/KykyRxrL3LS6oUm89)でも受け付けます

## 今日のテーマ

以下でGLMが分かる

* R入門 (30分)
    * 関数とベクトル、関数の最適化を理解する
* 8章と9章 (15分)
    * 因果(説明変数、応答変数)を理解する
* 10章 (30分)
    * 線形予測子とリンク関数を理解する

# R入門

## 今日のテーマ

* **R入門**
* 8章と9章
* 10章

```{r}
# 余白があるのでシードを与える
# 乱数なんかを固定できる
set.seed(1)
```
 
## R入門

* 関数と型、ベクトル
* 確率密度関数、尤度、パラメター
* 色々な最尤推定、関数の最適化

## 関数と型、ベクトル

確率密度関数って言ってるけど、そもそも**関数**ってなんだっけ？

* 「もらった要素（引数）に手を加えてから返すもの」です。
    
```{r}
factorial(5)
my.f <- function(x){
    x + 2
}
my.f(4)
```

## 関数と型、ベクトル

なら「型」ってなんですか？

* 文字型とか数字型とか色々あります。
* 関数の引数は特定の型しか受けつない場合があります。
    
```{r}
class("猫")
class(2)
# factorial("猫")
# Error in x + 1 : non-numeric argument to binary operator
```


## 関数と型、ベクトル

関数と型は分かったけど、「ベクトル」は？

* 全要素の型が同型と保証されています(atomic vector)。
* その型を引数にとる関数を全要素に適用(apply)できます！

```{r}
X <- 1:10
X
factorial(X)
# sapply(X, factorial)
```

## 関数と型、ベクトルのまとめ

ここまでのまとめ

* 関数は引数をとって操作を行ない結果を返す
* データには型があり、関数の引数には型の制限がある
* 同型である atomic vector には関数を apply できます。

えっと、じゃあ確率密度関数ってなに？

## 確率密度関数、パラメター、尤度

**確率密度関数** は任意の値をとって確率を返します...？
 
* 餌をばらまいたときに集まってくる猫の数
* 4匹集まる確率や10匹あつまる確率を返してくれる。

```{r, fig.height=3}
x <- 0:10  # 0から10までの atomic vector
dpois(lambda=4, 10) # 4匹集まる確率。lambda=4はとりあえず無視
p <- dpois(lambda=4, x) # xに確率密度関数`dpois`をapply
p # 各値が起きる確率のベクトル
```

## 確率密度関数、パラメター、尤度

**確率密度関数** は任意の値をとって確率を返します...？
 
* 餌をばらまいたときに集まってくる猫の数
* 4匹集まる確率や10匹あつまる確率を返してくれる。

```{r, fig.height=3}
plot(p~x, type="b", main="ポワソン分布(パラメターは4)")
```

## 確率密度関数、パラメター、尤度

確率密度関数を探る2つのアプローチ

* ~~0匹の確率、1匹の確率、とすべての条件で調べる~~
* 確率密度関数の **パラメター** を調べる （例: lambda=4）

```{r, fig.height=2.5}
layout(matrix(1:2, ncol=2))
plot(p~x, type="b", main="パラメターが4のポワソン分布")
p.lambda.8 <- dpois(lambda=8, x)  # もしパラメターが8だったら？
plot(p.lambda.8~x, type="b", main="パラメターが8のポワソン分布")
```

## 確率密度関数、パラメター、尤度

中央データはどっちのモデルから出てきたっぽい(=**尤度**)？

* パラメターを変化させて、最も尤度の高い値を探せば当たり

```{r, fig.height=2.5}
Y <- rpois(100, 4.3)
layout(matrix(1:3, ncol=3)) 
plot(p~x, type="b", main="パラメター 4のポワソン分布")
hist(Y)
plot(p.lambda.8~x, type="b", main="パラメター 8のポワソン分布")
```

## 確率密度関数、パラメター、尤度

パラメターを変化させて、最も尤度の高い値を探せば当たり

* $P(y|\theta=4)$ をすべてのyで求めて掛け算...（尤度）
* $\to$ 値が小さくなりすぎるので `Y -> log -> sum` （対数尤度）

```{r}
# Yそれぞれの確率を求める
p <- dpois(lambda=4, Y) # atomic vector Y をdpoisにマップしている
prod(p)  # 0.1, のような値を100個掛け合わせるのでめっちゃ小さくなってしまう
sum(log(p))  # 大小関係を見る際はlogをとってsumをとればよい。
```

## 確率密度関数、パラメター、尤度

パラメターが4の時の対数尤度は`r sum(log(p))` 。ほかの時は？

* Yが与えられているときにパラメター(λ)の尤度を求める関数を考える

```{r}
logL.f <- function(l){
    # Yは与えられていて、今度はlambda(l)を引数にとって尤度を返す
    Y.p <- dpois(lambda=l, Y)
    sum(log(Y.p))
}
# 先ほどと同じ値
logL.f(4)
```

## 確率密度関数、パラメター、尤度

* 1:10をパラメター(λ)に入れてそれぞれ求めてみる。

```{r, fig.height=3.5}
lambdas <- 1:10
logL <- sapply(lambdas, logL.f)
# lambda = 4 の時が一番尤度が高い <- **最尤推定値**
plot(logL~lambdas, type="l")
```

## 確率密度関数、パラメター、尤度

まとめ

* 確率密度関数は「ある値が実現する確率を返す関数」
* 「パラメター」で関数を定義してデータをモデル化
* データに対してモデル（with パラメター）は尤度を持つ
    * (尤度は掛け算で求められるけど、対数尤度を使う)

OK、プロットしたらわかるけど、最も尤もらしい値はどうやって求めるの？ <br>

$\to$ 最尤推定


## 最尤推定いろいろ

* 数式を解く(教科書pp. 152--153)
* `optim` 関数（このチュートリアルで結構使います）

```{r, fig.height=2.8}
plot(logL~lambdas, type="l")
```

## 最尤推定いろいろ

* 数式を解く(教科書pp. 152--153)
    * 尤度関数を微分して0の値(傾きが0の部分)

```{r, fig.height=2.8}
plot(logL~lambdas, type="l")
```

## 最尤推定いろいろ

* `optim` 関数で（このチュートリアルで結構使います）
    * Yはすでに分かっていて、未知のlambdaを知る

```{r, fig.height=2.8}
# うまく行くとき、行かないときがある。（データの数に依存）
logL.f <- function(parameters){
    lambda=parameters[1]
    Y <- rpois(1000, 4.3)
    Y.p <- dpois(lambda=lambda, Y)
    - sum(log(Y.p))
}
optim(c(1), fn = logL.f)$par # パラメターの初期値を1とする(0はできなかった)
```

## 最尤推定いろいろ

* `optim` 関数で（このチュートリアルで結構使います）
    * 推定する値は間接的でもよい

```{r, fig.height=2.5, fig.width=2.5}
logL.f <- function(parameters){
    beta = parameters[1] + 2
    lambda= beta
    Y <- rpois(1000, 4.3)
    -sum(log(dpois(lambda=lambda, Y)))
}
optim(c(2), fn = logL.f)$par
```

## 最尤推定のまとめ

* 数学的には対数尤度関数を微分して0になった点が最尤推定値
* `optim`関数でも実現できる
* 推定するパラメターとYの値にほかの操作が入っていてもOK

本当はMCMCなんかを使うとパラメターの確率分布、なんかも取れます。
そうすると、パラメターの値とそのばらつき、標準誤差なんかもわかります。
(今回は省きます。)

## 今日のテーマ

$f(y_i) = z_i =\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... \beta_kx_{ik} + \epsilon$

* **R入門** $\gets$ OK？
    * GLMってベクトルをとる関数？ $\gets$ そうです。
    * もしかして $\beta$ を最適化する流れ？ $\gets$ そうです。
    * この式の意味って？ $\gets$ こっからが本題です。
* 8章と9章
* 10章

質問は[こちら](https://forms.gle/KykyRxrL3LS6oUm89)から
